{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\p_k_s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\p_k_s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\p_k_s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\p_k_s\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries\n",
    "\n",
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Bearer token value from the environment variable\n",
    "bearer_token = os.getenv('BEARER_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Error occurred: 429 Too Many Requests\n",
      "88 - Rate limit exceeded\n",
      "Saved 89990 tweets to tweets.json\n"
     ]
    }
   ],
   "source": [
    "# Authenticate with Twitter API using API keys, API secret key, and Bearer token\n",
    "auth = tweepy.OAuth2BearerHandler(bearer_token)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Define the search query\n",
    "query = 'covid'\n",
    "\n",
    "# Define the fields to include in the response\n",
    "fields = ['created_at', 'id', 'full_text', 'user']\n",
    "\n",
    "# Extract data using search API\n",
    "tweets = []\n",
    "start_time = time.time()\n",
    "seconds = 1800\n",
    "while True:\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time >= seconds:\n",
    "        break\n",
    "    try:\n",
    "        for status in tweepy.Cursor(api.search_tweets,\n",
    "                                    q=query,\n",
    "                                    tweet_mode='extended',\n",
    "                                    lang='en',\n",
    "                                    result_type='recent',\n",
    "                                    count=100).items():\n",
    "             # Create a new dictionary with only the desired fields\n",
    "            tweet_data = {field: status._json[field] for field in fields}\n",
    "            tweets.append(tweet_data)\n",
    "            #print(elapsed_time)\n",
    "            #time.sleep(60) # Wait for 60 seconds and try again\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        time.sleep(30) # Wait for 60 seconds and try again\n",
    "\n",
    "# Save the extracted data as JSON\n",
    "with open('tweets.json', 'w') as f:\n",
    "    json.dump(tweets, f)\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"Saved {len(tweets)} tweets to tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming `tweets` is the list of dictionaries\n",
    "df = pd.DataFrame(tweets, columns=['id', 'created_at', 'full_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1634613559543083009</td>\n",
       "      <td>Sat Mar 11 17:53:36 +0000 2023</td>\n",
       "      <td>RT @CitizenFreePres: One thing I've learned ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1634613559484137474</td>\n",
       "      <td>Sat Mar 11 17:53:36 +0000 2023</td>\n",
       "      <td>RT @bambkb: Everything could cause death and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1634613556963430406</td>\n",
       "      <td>Sat Mar 11 17:53:36 +0000 2023</td>\n",
       "      <td>RT @Jim_Jordan: The House just voted unanimous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1634613555491377152</td>\n",
       "      <td>Sat Mar 11 17:53:35 +0000 2023</td>\n",
       "      <td>Don't forget we are offering FREE flu and COVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1634613554610601984</td>\n",
       "      <td>Sat Mar 11 17:53:35 +0000 2023</td>\n",
       "      <td>@ntsikasaunty Wow, he has lost a lot of weight...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                      created_at  \\\n",
       "0  1634613559543083009  Sat Mar 11 17:53:36 +0000 2023   \n",
       "1  1634613559484137474  Sat Mar 11 17:53:36 +0000 2023   \n",
       "2  1634613556963430406  Sat Mar 11 17:53:36 +0000 2023   \n",
       "3  1634613555491377152  Sat Mar 11 17:53:35 +0000 2023   \n",
       "4  1634613554610601984  Sat Mar 11 17:53:35 +0000 2023   \n",
       "\n",
       "                                           full_text  \n",
       "0  RT @CitizenFreePres: One thing I've learned ab...  \n",
       "1  RT @bambkb: Everything could cause death and i...  \n",
       "2  RT @Jim_Jordan: The House just voted unanimous...  \n",
       "3  Don't forget we are offering FREE flu and COVI...  \n",
       "4  @ntsikasaunty Wow, he has lost a lot of weight...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               full_text sentiment_category\n",
      "0      RT @CitizenFreePres: One thing I've learned ab...          very good\n",
      "1      RT @bambkb: Everything could cause death and i...           very bad\n",
      "2      RT @Jim_Jordan: The House just voted unanimous...               good\n",
      "3      Don't forget we are offering FREE flu and COVI...           very bad\n",
      "4      @ntsikasaunty Wow, he has lost a lot of weight...               good\n",
      "...                                                  ...                ...\n",
      "89985  Theresa Tam and all Public Health Officers &am...           very bad\n",
      "89986  @DonaldWelsh16 Indeed, the long-term problems ...           very bad\n",
      "89987  RT @EnemyInAState: Previous research has sugge...                bad\n",
      "89988  @elonmusk @RepMattGaetz @Timcast Someone doesn...                bad\n",
      "89989  TrustHealthInf: Frequently asked questions on ...           very bad\n",
      "\n",
      "[89990 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to clean tweets\n",
    "def clean_tweet(tweet):\n",
    "    # Remove Twitter-specific entities\n",
    "    tweet = re.sub(r'@[A-Za-z0-9_]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "\n",
    "    # Tokenize the tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if not token.lower() in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Convert tokens to lowercase and join them into a string\n",
    "    cleaned_tweet = \" \".join([token.lower() for token in lemmatized_tokens])\n",
    "\n",
    "    return cleaned_tweet\n",
    "\n",
    "\n",
    "# Clean the tweets\n",
    "df['cleaned_tweet'] = df['full_text'].apply(clean_tweet)\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Compute the sentiment scores for each tweet\n",
    "df['sentiment_scores'] = df['cleaned_tweet'].apply(lambda x: sia.polarity_scores(x))\n",
    "\n",
    "# Map the sentiment scores to categories\n",
    "def map_sentiment_score_to_category(sentiment_score):\n",
    "    if sentiment_score >= 0.6:\n",
    "        return 'very good'\n",
    "    elif sentiment_score >= 0.2:\n",
    "        return 'good'\n",
    "    elif sentiment_score > -0.2:\n",
    "        return 'neutral'\n",
    "    elif sentiment_score > -0.6:\n",
    "        return 'bad'\n",
    "    else:\n",
    "        return 'very bad'\n",
    "\n",
    "df['sentiment_category'] = df['sentiment_scores'].apply(lambda x: map_sentiment_score_to_category(x['compound']))\n",
    "\n",
    "# Print the results\n",
    "print(df[['full_text', 'sentiment_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>sentiment_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1634613559543083009</td>\n",
       "      <td>Sat Mar 11 17:53:36 +0000 2023</td>\n",
       "      <td>RT @CitizenFreePres: One thing I've learned ab...</td>\n",
       "      <td>rt one thing ive learned guy last 20 year love...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.756, 'pos': 0.244, 'comp...</td>\n",
       "      <td>very good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1634613559484137474</td>\n",
       "      <td>Sat Mar 11 17:53:36 +0000 2023</td>\n",
       "      <td>RT @bambkb: Everything could cause death and i...</td>\n",
       "      <td>rt everything could cause death injury except ...</td>\n",
       "      <td>{'neg': 0.456, 'neu': 0.544, 'pos': 0.0, 'comp...</td>\n",
       "      <td>very bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1634613556963430406</td>\n",
       "      <td>Sat Mar 11 17:53:36 +0000 2023</td>\n",
       "      <td>RT @Jim_Jordan: The House just voted unanimous...</td>\n",
       "      <td>rt house voted unanimously declassify intellig...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.744, 'pos': 0.256, 'comp...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1634613555491377152</td>\n",
       "      <td>Sat Mar 11 17:53:35 +0000 2023</td>\n",
       "      <td>Don't forget we are offering FREE flu and COVI...</td>\n",
       "      <td>dont forget offering free flu covid19 vaccinat...</td>\n",
       "      <td>{'neg': 0.324, 'neu': 0.604, 'pos': 0.072, 'co...</td>\n",
       "      <td>very bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1634613554610601984</td>\n",
       "      <td>Sat Mar 11 17:53:35 +0000 2023</td>\n",
       "      <td>@ntsikasaunty Wow, he has lost a lot of weight...</td>\n",
       "      <td>wow lost lot weight covid something else</td>\n",
       "      <td>{'neg': 0.207, 'neu': 0.45, 'pos': 0.342, 'com...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                      created_at  \\\n",
       "0  1634613559543083009  Sat Mar 11 17:53:36 +0000 2023   \n",
       "1  1634613559484137474  Sat Mar 11 17:53:36 +0000 2023   \n",
       "2  1634613556963430406  Sat Mar 11 17:53:36 +0000 2023   \n",
       "3  1634613555491377152  Sat Mar 11 17:53:35 +0000 2023   \n",
       "4  1634613554610601984  Sat Mar 11 17:53:35 +0000 2023   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  RT @CitizenFreePres: One thing I've learned ab...   \n",
       "1  RT @bambkb: Everything could cause death and i...   \n",
       "2  RT @Jim_Jordan: The House just voted unanimous...   \n",
       "3  Don't forget we are offering FREE flu and COVI...   \n",
       "4  @ntsikasaunty Wow, he has lost a lot of weight...   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  rt one thing ive learned guy last 20 year love...   \n",
       "1  rt everything could cause death injury except ...   \n",
       "2  rt house voted unanimously declassify intellig...   \n",
       "3  dont forget offering free flu covid19 vaccinat...   \n",
       "4           wow lost lot weight covid something else   \n",
       "\n",
       "                                    sentiment_scores sentiment_category  \n",
       "0  {'neg': 0.0, 'neu': 0.756, 'pos': 0.244, 'comp...          very good  \n",
       "1  {'neg': 0.456, 'neu': 0.544, 'pos': 0.0, 'comp...           very bad  \n",
       "2  {'neg': 0.0, 'neu': 0.744, 'pos': 0.256, 'comp...               good  \n",
       "3  {'neg': 0.324, 'neu': 0.604, 'pos': 0.072, 'co...           very bad  \n",
       "4  {'neg': 0.207, 'neu': 0.45, 'pos': 0.342, 'com...               good  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
